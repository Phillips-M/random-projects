{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression is a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The purpose of this tutorial is to explain what some may be considered a complex idea in machine learning: neural networks. Here, the reader will see that logistic regression can be framed as a specific architecture of a neural network. Specifically, a logistic regression model is a neural network with no hidden layers, a particular optimization method, without minibatches, and a slightly altered loss function.\n",
    "\n",
    "\n",
    "The target reader is the data analytics professional who may not consider themself a machine learning practitioner.  The only prerequisites are only basic linear algebra and calculus. Familiarity with certain concepts related to neural networks made in this article is not required but the reader is encouraged to familiarize themselves if they are not already familiar using the provided links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will not appeal to visual representations of neurons like the below image; rather, it will utilize basic concepts in linear algebra, probability, and calculus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/296px-Colored_neural_network.svg.png)\n",
    "Source: Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, tutorial is in R in order to drive home the fact that tools which data analytics professionals use on a regular basis (namely, `glm`), leverage the same concepts 'under the hood' that power machine learning methods like neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To run this tutorial in your own R environment, you will need the following R CRAN libraries: \n",
    "\n",
    "-  [Keras](https://cran.r-project.org/web/packages/keras/index.html)\n",
    "\n",
    "Make sure the required packages are installed if they aren't already. [Keras](https://keras.io/) is a machine learning library that is a wrapper  to other deep learning frameworks such as TensorFlow and Theano. Here we import the R interface to Keras. Note that this post will not cover the Keras API; however, code snippets are provided. \n",
    "\n",
    "The script below will help you verify that the packages are installed. If they are not, the command `install.packages()` will install them. Be advised that the libraries have dependencies that will also be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (!(\"keras\" %in% installed.packages()[, \"Package\"])) {\n",
    "    install.packages(package)\n",
    "}\n",
    "\n",
    "library(keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the insights in this post are not specific to any particular dataset, we can generate a random Gaussian features with a random Bernoulli target. We will initialize the matrix $X$ as having 1000 rows and 10 columns and array $Y$ as having 1000 rows. Keep these dimensions in mind for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(42)\n",
    "\n",
    "X <- matrix(rnorm(10 * 1000, mean = 1, sd = 0.5), ncol=10)\n",
    "Y <- matrix(rbinom(1000, 1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader is free to rename the features of the dataset but as the particular names aren't important as the values of the estimated coefficients, the features will remain nameless for this demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.frame(X)\n",
    "data$target = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>X1</th><th scope=col>X2</th><th scope=col>X3</th><th scope=col>X4</th><th scope=col>X5</th><th scope=col>X6</th><th scope=col>X7</th><th scope=col>X8</th><th scope=col>X9</th><th scope=col>X10</th><th scope=col>target</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1.6854792   </td><td>2.1625292   </td><td> 1.125289034</td><td>0.6571692   </td><td>0.929095672 </td><td>1.0356112   </td><td>1.0864161   </td><td>1.7081707   </td><td>0.9712737   </td><td> 0.5389325  </td><td>1           </td></tr>\n",
       "\t<tr><td>0.7176509   </td><td>1.2620611   </td><td> 0.861037975</td><td>0.6036428   </td><td>0.593050963 </td><td>1.4851450   </td><td>0.3635182   </td><td>1.2786170   </td><td>0.8754823   </td><td> 0.7520914  </td><td>0           </td></tr>\n",
       "\t<tr><td>1.1815642   </td><td>1.4853667   </td><td> 0.137632133</td><td>0.7964979   </td><td>0.837229705 </td><td>1.1550176   </td><td>0.5660523   </td><td>1.4906207   </td><td>0.2379189   </td><td>-0.5552311  </td><td>0           </td></tr>\n",
       "\t<tr><td>1.3164313   </td><td>1.1884867   </td><td>-0.003352472</td><td>0.4256647   </td><td>1.189078699 </td><td>0.9302257   </td><td>1.3131606   </td><td>0.7069086   </td><td>1.2317955   </td><td> 0.6536197  </td><td>1           </td></tr>\n",
       "\t<tr><td>1.2021342   </td><td>0.5020333   </td><td> 0.354095835</td><td>1.5578802   </td><td>0.002757319 </td><td>0.8368444   </td><td>0.9471847   </td><td>1.4695853   </td><td>0.4061896   </td><td> 1.1494506  </td><td>0           </td></tr>\n",
       "\t<tr><td>0.9469377   </td><td>0.7012585   </td><td> 1.182919114</td><td>0.5602716   </td><td>0.500321653 </td><td>0.9405952   </td><td>0.8718393   </td><td>0.9676495   </td><td>1.2470321   </td><td> 0.9656635  </td><td>0           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       " X1 & X2 & X3 & X4 & X5 & X6 & X7 & X8 & X9 & X10 & target\\\\\n",
       "\\hline\n",
       "\t 1.6854792    & 2.1625292    &  1.125289034 & 0.6571692    & 0.929095672  & 1.0356112    & 1.0864161    & 1.7081707    & 0.9712737    &  0.5389325   & 1           \\\\\n",
       "\t 0.7176509    & 1.2620611    &  0.861037975 & 0.6036428    & 0.593050963  & 1.4851450    & 0.3635182    & 1.2786170    & 0.8754823    &  0.7520914   & 0           \\\\\n",
       "\t 1.1815642    & 1.4853667    &  0.137632133 & 0.7964979    & 0.837229705  & 1.1550176    & 0.5660523    & 1.4906207    & 0.2379189    & -0.5552311   & 0           \\\\\n",
       "\t 1.3164313    & 1.1884867    & -0.003352472 & 0.4256647    & 1.189078699  & 0.9302257    & 1.3131606    & 0.7069086    & 1.2317955    &  0.6536197   & 1           \\\\\n",
       "\t 1.2021342    & 0.5020333    &  0.354095835 & 1.5578802    & 0.002757319  & 0.8368444    & 0.9471847    & 1.4695853    & 0.4061896    &  1.1494506   & 0           \\\\\n",
       "\t 0.9469377    & 0.7012585    &  1.182919114 & 0.5602716    & 0.500321653  & 0.9405952    & 0.8718393    & 0.9676495    & 1.2470321    &  0.9656635   & 0           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8 | X9 | X10 | target | \n",
       "|---|---|---|---|---|---|\n",
       "| 1.6854792    | 2.1625292    |  1.125289034 | 0.6571692    | 0.929095672  | 1.0356112    | 1.0864161    | 1.7081707    | 0.9712737    |  0.5389325   | 1            | \n",
       "| 0.7176509    | 1.2620611    |  0.861037975 | 0.6036428    | 0.593050963  | 1.4851450    | 0.3635182    | 1.2786170    | 0.8754823    |  0.7520914   | 0            | \n",
       "| 1.1815642    | 1.4853667    |  0.137632133 | 0.7964979    | 0.837229705  | 1.1550176    | 0.5660523    | 1.4906207    | 0.2379189    | -0.5552311   | 0            | \n",
       "| 1.3164313    | 1.1884867    | -0.003352472 | 0.4256647    | 1.189078699  | 0.9302257    | 1.3131606    | 0.7069086    | 1.2317955    |  0.6536197   | 1            | \n",
       "| 1.2021342    | 0.5020333    |  0.354095835 | 1.5578802    | 0.002757319  | 0.8368444    | 0.9471847    | 1.4695853    | 0.4061896    |  1.1494506   | 0            | \n",
       "| 0.9469377    | 0.7012585    |  1.182919114 | 0.5602716    | 0.500321653  | 0.9405952    | 0.8718393    | 0.9676495    | 1.2470321    |  0.9656635   | 0            | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  X1        X2        X3           X4        X5          X6        X7       \n",
       "1 1.6854792 2.1625292  1.125289034 0.6571692 0.929095672 1.0356112 1.0864161\n",
       "2 0.7176509 1.2620611  0.861037975 0.6036428 0.593050963 1.4851450 0.3635182\n",
       "3 1.1815642 1.4853667  0.137632133 0.7964979 0.837229705 1.1550176 0.5660523\n",
       "4 1.3164313 1.1884867 -0.003352472 0.4256647 1.189078699 0.9302257 1.3131606\n",
       "5 1.2021342 0.5020333  0.354095835 1.5578802 0.002757319 0.8368444 0.9471847\n",
       "6 0.9469377 0.7012585  1.182919114 0.5602716 0.500321653 0.9405952 0.8718393\n",
       "  X8        X9        X10        target\n",
       "1 1.7081707 0.9712737  0.5389325 1     \n",
       "2 1.2786170 0.8754823  0.7520914 0     \n",
       "3 1.4906207 0.2379189 -0.5552311 0     \n",
       "4 0.7069086 1.2317955  0.6536197 1     \n",
       "5 1.4695853 0.4061896  1.1494506 0     \n",
       "6 0.9676495 1.2470321  0.9656635 0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the terms weights and coefficients will be used interchangeably with the math terms $\\theta$ and $\\beta$, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s worth pausing now to consider the dimensions of the data arrays we’re going to use for the model. The actual dimensions aren’t as important as the formalization of the dimensions, which are as follows:\n",
    "\n",
    "- __N__: Number of rows (observations)\n",
    "- __K__: Number of columns (features)\n",
    "\n",
    "In a standard logistic regression setting, where there is also an intercept that is estimated in addition to the coefficients, the dimensionality of the input data is $N \\times (K + 1)$. However, for this demonstration we won't be estimating an intercept for simplicity. \n",
    "\n",
    "Therefore, __the dimension of our input data $X$ can be formalized as $N \\times K$__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1000</li>\n",
       "\t<li>10</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1000\n",
       "\\item 10\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1000\n",
       "2. 10\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1000   10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The dimension of our output data or target $Y$ is $N \\times 1$__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1000</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1000\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1000\n",
       "2. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1000    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generalized linear models, problem is formalized as $\\hat{Y} = f(X\\beta)$ (where $\\beta$ is our coefficient matrix). \n",
    "\n",
    "- In a __linear regression__ setting, $f$ is the identity function so __$\\hat{Y} = X\\beta$__. \n",
    "\n",
    "- In a __logistic regression setting__, $f$ is the sigmoid function, defined as $\\frac{1}{1 + \\exp^{-x}}$, so __$\\hat{Y} = sigmoid(X\\beta)$__.\n",
    "\n",
    "Recall from linear algebra that in order to obtain the dot product of two matrices, the dimension of the first matrix's columns must match the dimension of the second matrix's rows. \n",
    "\n",
    "It follows that in our example, __the dimension of the coefficient matrix $\\beta$ is $K \\times 1$__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a 1 Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full disclosure: all of the hyperparameters were carefully chosen to produce the same weights in the NN model as the logistic regression model. It is by no means guaranteed to get similar weights for the 2 types of models across different hyperparameters or even across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further ado, let's dive right in and build a neural network model using Keras.\n",
    "\n",
    "$1$. We first __define the input layer__, specifying that the shape of the input data is equal to the number of columns of the dataset $X$. Notice that we don't have to define the number of rows $N$. This is because while training the neural network, we can control the number of rows used to train the model in each epoch or iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs <- layer_input(shape = c(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider why we used 10 in the `shape` argument of our input layer. It corresponds to the column space of our dataset $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Since this model is only 1 layer (ignoring the input layer), we next __define the output layer__ which is the only layer with trainable weights. We use a sigmoid [activation function](https://en.wikipedia.org/wiki/Activation_function) which is essentially $f$ from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs <- inputs %>%\n",
    "    layer_dense(units = 1, kernel_initializer = initializer_constant(value = 0), activation = 'sigmoid', use_bias = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from above the dimensions of target $Y$: $N \\times 1$. Similar to above, the value in the `units` argument in `layer_dense` corresponds to the columns space of our target $Y$.\n",
    "\n",
    "Using matrix multiplication we can describe at a very high level what is going on:\n",
    "\n",
    "`layer_input` (dimension: $? \\times K)$ `%*%` `layer_dense` (dimension: $K \\times 1$) `%>%` `sigmoid_function` -> `outputs` (dimension: $? \\times 1$)\n",
    "\n",
    "-  `%*%` is R syntax for matrix multiplication\n",
    "-  `%>%` is `dplyr` syntax for piping the output of the previous function as input to the following function\n",
    "\n",
    "$3$. Finally, we __define the overall structure of the model__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model <- keras_model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. The model has to be compiled with a few hyperparameters that we set. We __compile our model__ using `binary_crossentropy` as the loss function to minimize, which is explained below. We also define `Adam` as its optimization method. A brief discussion of optimization methods follows later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model %>% compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = c('binary_crossentropy')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is binary crossentropy? Binary cross-entropy is defined as:\n",
    "\n",
    "$$\\large - \\Sigma_{i=1}^{n} \\big( y_i \\ log(\\hat{p}) + (1 - y_i) \\ log(1 - \\hat{p}) \\big)$$\n",
    "\n",
    "- $y_i$ is each _observed_ label\n",
    "\n",
    "- $\\hat{p}$ is the _predicted_ probability that $y_i = 1$. It's calculated as the output of the sigmoid function $\\frac{1}{1 + \\exp^{- X \\theta}}$. You may also find it as $\\hat{y}$ in other online resources.\n",
    "\n",
    "\n",
    "In a logistic regression setting, we are interested in doing maximum likelihood estimation using the Bernoulli likelihood function, which is defined as:\n",
    "\n",
    "$$\\large \\Pi_{i=1}^{n} \\big( \\hat{p}^{y_i} \\ (1 - \\hat{p})^{(1 - y_i)} \\big) $$\n",
    "\n",
    "\n",
    "However, taking the log of the above makes the problem more computationally tractable. \n",
    "\n",
    "Recall that:\n",
    "- $log (a * b) = log(a) + log(b)$\n",
    "\n",
    "- $log(b^{a}) = b \\ log(a)$.\n",
    "\n",
    "So taking the log of the above we obtain the Bernoulli log-likelihood function, defined as:\n",
    "\n",
    "$$ \\large \\Sigma_{i=1}^{n} \\big( y_i \\ log(\\hat{p}) + (1 - y_i) \\ log(1-\\hat{p}) \\big)$$\n",
    "\n",
    "\n",
    "\n",
    "The two functions are identitical except for the sign reversal! In a neural network setting, we are trying to _minimize_ binary cross-entropy while in a logistic regression setting we are trying to _maximize_ Bernoulli log-likelihood (aka [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- nn_model %>% fit(\n",
    "    X, Y, \n",
    "    epochs = 1000, batch_size = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  `epochs` is the number of passes over the entire dataset.\n",
    "-  `batch_size` is the number of samples out of the original data set we're using to run optimization at one time. It's considered good practice to use a value that's a fraction of the size of your data but in this example we're using the entire dataset at once to mimic logistic regression.\n",
    "\n",
    "Next, we can get the weights from the model. In our model there is only 1 trainable layer so we only see an array of dimension $K \\times 1$, just as defined above in `layer_dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<tbody>\n",
       "\t<tr><td>-0.003036723</td></tr>\n",
       "\t<tr><td>-0.005477559</td></tr>\n",
       "\t<tr><td> 0.149673387</td></tr>\n",
       "\t<tr><td>-0.211892888</td></tr>\n",
       "\t<tr><td> 0.006067039</td></tr>\n",
       "\t<tr><td>-0.126188636</td></tr>\n",
       "\t<tr><td> 0.023240240</td></tr>\n",
       "\t<tr><td> 0.038955178</td></tr>\n",
       "\t<tr><td> 0.108082101</td></tr>\n",
       "\t<tr><td> 0.118358888</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{tabular}{l}\n",
       "\t -0.003036723\\\\\n",
       "\t -0.005477559\\\\\n",
       "\t  0.149673387\\\\\n",
       "\t -0.211892888\\\\\n",
       "\t  0.006067039\\\\\n",
       "\t -0.126188636\\\\\n",
       "\t  0.023240240\\\\\n",
       "\t  0.038955178\\\\\n",
       "\t  0.108082101\\\\\n",
       "\t  0.118358888\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "| -0.003036723 | \n",
       "| -0.005477559 | \n",
       "|  0.149673387 | \n",
       "| -0.211892888 | \n",
       "|  0.006067039 | \n",
       "| -0.126188636 | \n",
       "|  0.023240240 | \n",
       "|  0.038955178 | \n",
       "|  0.108082101 | \n",
       "|  0.118358888 | \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "              [,1]\n",
       " [1,] -0.003036723\n",
       " [2,] -0.005477559\n",
       " [3,]  0.149673387\n",
       " [4,] -0.211892888\n",
       " [5,]  0.006067039\n",
       " [6,] -0.126188636\n",
       " [7,]  0.023240240\n",
       " [8,]  0.038955178\n",
       " [9,]  0.108082101\n",
       "[10,]  0.118358888\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_weights(nn_model)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap so far: we’ve trained a 1 layer neural network. Since the target variable is binary, we used `binary_crossentropy` as the loss function. We used [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) as the optimizer. After training the model we extracted the weights from the single trainable layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to logistic regression by fitting a logistic regression model to the data, leaving out the intercept as with the neural network. Just like with other machine learning methods, logistic regression in R is doing some optimization under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model <- glm(Y ~ X - 1, family = binomial(link='logit'), control = list(maxit = 25, epsilon=1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we obtained the weights from the neural network, we can obtain the coefficients from the logistic regression model for comparison to the weights from the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>X1</dt>\n",
       "\t\t<dd>-0.00303674800956899</dd>\n",
       "\t<dt>X2</dt>\n",
       "\t\t<dd>-0.00547757226083272</dd>\n",
       "\t<dt>X3</dt>\n",
       "\t\t<dd>0.149673635557747</dd>\n",
       "\t<dt>X4</dt>\n",
       "\t\t<dd>-0.211893223812615</dd>\n",
       "\t<dt>X5</dt>\n",
       "\t\t<dd>0.00606699476274347</dd>\n",
       "\t<dt>X6</dt>\n",
       "\t\t<dd>-0.126188515878772</dd>\n",
       "\t<dt>X7</dt>\n",
       "\t\t<dd>0.0232402295295685</dd>\n",
       "\t<dt>X8</dt>\n",
       "\t\t<dd>0.0389551593501125</dd>\n",
       "\t<dt>X9</dt>\n",
       "\t\t<dd>0.108082121793733</dd>\n",
       "\t<dt>X10</dt>\n",
       "\t\t<dd>0.11835894529033</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[X1] -0.00303674800956899\n",
       "\\item[X2] -0.00547757226083272\n",
       "\\item[X3] 0.149673635557747\n",
       "\\item[X4] -0.211893223812615\n",
       "\\item[X5] 0.00606699476274347\n",
       "\\item[X6] -0.126188515878772\n",
       "\\item[X7] 0.0232402295295685\n",
       "\\item[X8] 0.0389551593501125\n",
       "\\item[X9] 0.108082121793733\n",
       "\\item[X10] 0.11835894529033\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "X1\n",
       ":   -0.00303674800956899X2\n",
       ":   -0.00547757226083272X3\n",
       ":   0.149673635557747X4\n",
       ":   -0.211893223812615X5\n",
       ":   0.00606699476274347X6\n",
       ":   -0.126188515878772X7\n",
       ":   0.0232402295295685X8\n",
       ":   0.0389551593501125X9\n",
       ":   0.108082121793733X10\n",
       ":   0.11835894529033\n",
       "\n"
      ],
      "text/plain": [
       "          X1           X2           X3           X4           X5           X6 \n",
       "-0.003036748 -0.005477572  0.149673636 -0.211893224  0.006066995 -0.126188516 \n",
       "          X7           X8           X9          X10 \n",
       " 0.023240230  0.038955159  0.108082122  0.118358945 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic_model$coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do they look similar to the weights from the neural network model above? Let's find the mean absolute percentage error of the neural network weights from the logistic regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error between NN model and LR:  0.0002394773"
     ]
    }
   ],
   "source": [
    "nn_weights <- get_weights(nn_model)[[1]]\n",
    "lr_coefs <- logistic_model$coefficients\n",
    "\n",
    "cat(\"Mean absolute percentage error between NN model and LR: \", 100 * mean(abs((lr_coefs - nn_weights) / lr_coefs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients from the logistic regression model and the weights from the neural network model are very similar, but not exactly the same. Any differences might be attributable to any number of things, but most probably the different optimization methods used in both models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side Discussion on Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pause to highlight perhaps the biggest difference between logistic regression our neural network: the optimization method. The links provided in this section will be more challenging. \n",
    "\n",
    "R uses [Iteratively reweighted least squares](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares) to solve for the coefficient values of logistic regression. IRLS is the algorithm implementation of [Fisher's scoring](https://en.wikipedia.org/wiki/Scoring_algorithm), a variant of [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method), which is a _second order method_ as it makes use of the second derivative of the loss function.  \n",
    "\n",
    "All of the commonly used optimization methods used for deep neural networks, such as `Adam` here are _first order methods_. First order methods rather than second order methods are used for deep neural networks because the loss functions are almost certainly not [convex](https://en.wikipedia.org/wiki/Convex_function) and computing the second order derivatives would be extremely time consuming and computationally expensive. \n",
    "\n",
    "Logistic regression can make use of second order methods because the log-likelihood function is a concave function, which guarantees that there is at most one maximum for the function. Side note: It is possible there is no global maximum for the function. The reader has perhaps encountered problems where there has not been convergence for their logistic regression model. For example, when there is multicollinarity between predictor values or when the predictor values perfectly predict the target variable ([complete separation](https://en.wikipedia.org/wiki/Separation_(statistics)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-building the Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you're still skeptical that logistic regression is the same as a 1 layer neural network. So far we've built a 1 layer neural network model in Keras with binary cross-entropy as the loss function and we've also demonstrated that binary cross-entropy and Bernoulli log-likelihood are equivalent except for a loss function. To close the loop let's implement our neural network model from above from scratch, with one exception: instead of minimizing binary crossentropy we're going to maximize log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that \n",
    "\n",
    "$$binary \\ crossentropy = \\large - \\Sigma_{i=1}^{n} \\big( y_i \\ (log(\\hat{p})) + (1 - y_i) \\ log(1 - \\hat{p}) \\big)$$\n",
    "\n",
    "and since $\\hat{p} = \\frac{1}{1 + \\exp^{- X \\theta}}$, we obtain\n",
    "\n",
    "$$\\frac{\\partial \\ binary \\ crossentropy}{\\partial \\ \\theta} = \\large \\Sigma_{i=1}^{n} \\big( \\hat{p} - y_i \\big) x_i$$\n",
    "\n",
    "The derivative of the log-likelihood with respect to $\\theta$ is the negative of the above. The generalization of this concept applied to loss functions of deep neural networks is called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implements the `Adam` optimizer from the original paper [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). The choices of hyperparameters are taken from the default Keras implementation of `Adam` which are identical to those chosen by the paper's authors. In-depth understanding of the optimizer is not necessary and the below is only given for a demonstration that __if you switch the sign of the original loss function (binary cross-entropy to Bernoulli log-likelihood) and update weights accordingly, you again get weights that are similar to the logistic regression coefficients__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N <- dim(X)[1]\n",
    "K <- dim(X)[2]\n",
    "n_epochs <- 1000\n",
    "batch_size <- 1000\n",
    "\n",
    "\n",
    "sigmoid_function <- function(z) {\n",
    "    return (1 / (1 + exp(-z)))\n",
    "}\n",
    "\n",
    "# The following is a direct implementation of Adam per the paper above. The reader is not expected\n",
    "# to develop intuition for the algorithm; rather, it's presented only as a more explicit demonstration\n",
    "# of the Keras implementation of the neural network model.\n",
    "\n",
    "# Initialize weight vector theta\n",
    "theta <- rep(0, K)\n",
    "\n",
    "# Initialize hyperparameters\n",
    "alpha <- 0.001\n",
    "beta1 <- 0.9\n",
    "beta2 <- 0.999\n",
    "\n",
    "# Initialize 1st moment vector\n",
    "m <- 0\n",
    "\n",
    "# Initialize 2nd moment vector\n",
    "v <- 0\n",
    "\n",
    "# Keras default epsilon value but different from paper\n",
    "eps <- 1e-7\n",
    "\n",
    "for (epoch in 1:n_epochs) {\n",
    "    \n",
    "    # Similar to batch from above. Note that we also randomize the indexes of the minibatches.\n",
    "    for (batch in split(sample(seq(1, length(Y))), ceiling(seq_along(seq(1, length(Y))) / batch_size))) {\n",
    "    \n",
    "        z <- X[batch,] %*% theta\n",
    "        p_hat <- sigmoid_function(z)\n",
    "\n",
    "        # In a normal neural network setting we would use the derivative of binary cross-entropy, but we're\n",
    "        # using the derivative of the log-likelihood here. Remember they are identical except for a sign reversal.\n",
    "\n",
    "        # deriv_binary_crossentropy <- t(X) %*% (y_hat - Y)\n",
    "        deriv_log_likelihood <-  - t(X[batch,]) %*% (p_hat - Y[batch,])\n",
    "        \n",
    "        # Update biased first order estimate\n",
    "        m <- beta1 * m + (1 - beta1) * deriv_log_likelihood\n",
    "    \n",
    "        # Update biased second raw moment estimate\n",
    "        v <- beta2 * v + (1 - beta2) * deriv_log_likelihood^2\n",
    "        \n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_hat <- m / (1 - beta1^epoch)\n",
    "        \n",
    "        # Compute bias-corrected second raw moment estimate\n",
    "        v_hat <- v / (1 - beta2^epoch)\n",
    "\n",
    "        # Update parameters. Remember that we are adding the update because we are maximizing \n",
    "        # Bernoulli log-likelihood.\n",
    "        theta <- theta + alpha * (m_hat / (sqrt(v_hat) + eps))\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percentage error between new NN model and LR:  0.000326558"
     ]
    }
   ],
   "source": [
    "cat(\"Mean absolute percentage error between new NN model and LR: \", 100 * mean(abs((lr_coefs - theta) / lr_coefs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap what we've done here:\n",
    "\n",
    "-  We built a 1 layer neural network model using popular deep learning library `keras` and a logitic regression model using `glm` in R.\n",
    "- We've seen that the weights from the neural network model we built are very similar to the coefficients obtained from a logistic regression model. \n",
    "-  We've seen how Bernoulli log-likelihood used in logistic regression is just binary cross-entropy with a sign reversal.\n",
    "- We built the same neural network model 'from scratch' except that we switched the sign of the loss function and obtained weights that are very similar to the logistic regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum: __logistic regression is a neural network!__\n",
    "\n",
    "If you've used logistic regression in R before, you've used a neural network. (Though you probably shouldn't change your LinkedIn profile to Deep Learning Engineer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  [What is the relation between Logistic Regression and Neural Networks and when to use which?](https://sebastianraschka.com/faq/docs/logisticregr-neuralnet.html)\n",
    "-  [Single-Layer Neural Networks and Gradient Descent](http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html)\n",
    "-  [A Statistical View of Deep Learning (I): Recursive GLMs](http://blog.shakirm.com/2015/01/a-statistical-view-of-deep-learning-i-recursive-glms/)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
