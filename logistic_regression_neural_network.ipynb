{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression is a 1 Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The purpose of this tutorial is to demystify an increasingly popular method in machine learning: neural networks. Here, the reader will see that logistic regression can be framed as a specific architecture of a neural network. \n",
    "\n",
    "## The tutorial is presented in R in an effort to appeal to data professionals who may not consider themselves machine learning practitioners. The only prerequisites are experience with logistic regression and familiarity with calculus concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the required packages are installed if they aren't already.\n",
    "\n",
    "for (package in c(\"keras\", \"mlbench\")) {\n",
    "    if (!(package %in% installed.packages()[, \"Package\"])) {\n",
    "        install.packages(package)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Keras](https://keras.io/) is a machine learning library that is a front-end to other deep learning frameworks such as TensorFlow and Theano. Here we import the R interface to Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(mlbench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a sample dataset in. Here we're using the [Pima Indians Diabetes Data Set ](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>pregnant</th><th scope=col>glucose</th><th scope=col>pressure</th><th scope=col>triceps</th><th scope=col>insulin</th><th scope=col>mass</th><th scope=col>pedigree</th><th scope=col>age</th><th scope=col>diabetes</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>6    </td><td>148  </td><td>72   </td><td>35   </td><td>  0  </td><td>33.6 </td><td>0.627</td><td>50   </td><td>pos  </td></tr>\n",
       "\t<tr><td>1    </td><td> 85  </td><td>66   </td><td>29   </td><td>  0  </td><td>26.6 </td><td>0.351</td><td>31   </td><td>neg  </td></tr>\n",
       "\t<tr><td>8    </td><td>183  </td><td>64   </td><td> 0   </td><td>  0  </td><td>23.3 </td><td>0.672</td><td>32   </td><td>pos  </td></tr>\n",
       "\t<tr><td>1    </td><td> 89  </td><td>66   </td><td>23   </td><td> 94  </td><td>28.1 </td><td>0.167</td><td>21   </td><td>neg  </td></tr>\n",
       "\t<tr><td>0    </td><td>137  </td><td>40   </td><td>35   </td><td>168  </td><td>43.1 </td><td>2.288</td><td>33   </td><td>pos  </td></tr>\n",
       "\t<tr><td>5    </td><td>116  </td><td>74   </td><td> 0   </td><td>  0  </td><td>25.6 </td><td>0.201</td><td>30   </td><td>neg  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " pregnant & glucose & pressure & triceps & insulin & mass & pedigree & age & diabetes\\\\\n",
       "\\hline\n",
       "\t 6     & 148   & 72    & 35    &   0   & 33.6  & 0.627 & 50    & pos  \\\\\n",
       "\t 1     &  85   & 66    & 29    &   0   & 26.6  & 0.351 & 31    & neg  \\\\\n",
       "\t 8     & 183   & 64    &  0    &   0   & 23.3  & 0.672 & 32    & pos  \\\\\n",
       "\t 1     &  89   & 66    & 23    &  94   & 28.1  & 0.167 & 21    & neg  \\\\\n",
       "\t 0     & 137   & 40    & 35    & 168   & 43.1  & 2.288 & 33    & pos  \\\\\n",
       "\t 5     & 116   & 74    &  0    &   0   & 25.6  & 0.201 & 30    & neg  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "pregnant | glucose | pressure | triceps | insulin | mass | pedigree | age | diabetes | \n",
       "|---|---|---|---|---|---|\n",
       "| 6     | 148   | 72    | 35    |   0   | 33.6  | 0.627 | 50    | pos   | \n",
       "| 1     |  85   | 66    | 29    |   0   | 26.6  | 0.351 | 31    | neg   | \n",
       "| 8     | 183   | 64    |  0    |   0   | 23.3  | 0.672 | 32    | pos   | \n",
       "| 1     |  89   | 66    | 23    |  94   | 28.1  | 0.167 | 21    | neg   | \n",
       "| 0     | 137   | 40    | 35    | 168   | 43.1  | 2.288 | 33    | pos   | \n",
       "| 5     | 116   | 74    |  0    |   0   | 25.6  | 0.201 | 30    | neg   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n",
       "1 6        148     72       35        0     33.6 0.627    50  pos     \n",
       "2 1         85     66       29        0     26.6 0.351    31  neg     \n",
       "3 8        183     64        0        0     23.3 0.672    32  pos     \n",
       "4 1         89     66       23       94     28.1 0.167    21  neg     \n",
       "5 0        137     40       35      168     43.1 2.288    33  pos     \n",
       "6 5        116     74        0        0     25.6 0.201    30  neg     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(PimaIndiansDiabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace variable 'diabetes' with binary {0, 1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PimaIndiansDiabetes$diabetes <- ifelse(PimaIndiansDiabetes$diabetes==\"pos\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- PimaIndiansDiabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- as.matrix(data[, seq(1,8)])\n",
    "Y <- as.matrix(data[, 9])\n",
    "\n",
    "\n",
    "# Below is optional for the given exercise.\n",
    "sample_size <- floor(0.75 * nrow(data))\n",
    "train_ind <- sample(seq_len(nrow(data)), size = sample_size)\n",
    "x_train <- X[train_ind, ]\n",
    "y_train <- Y[train_ind, ]\n",
    "x_test <- X[-train_ind, ]\n",
    "y_test <- Y[-train_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim(data) = n x k\n",
    "# dim(weights) = k x 1\n",
    "# dim(output) = n x 1\n",
    "\n",
    "# dim(X)[2] = k\n",
    "inputs <- layer_input(shape = c(dim(X)[2]))\n",
    " \n",
    "# Model will be only 1 layer (besides input) -- the weight layer and the output (argument in layer_dense)\n",
    "outputs <- inputs %>%\n",
    "    # activation is sigmoid because we're dealing with logistic regression. It's the same as \n",
    "    # logit function -> 1 / (1 + exp(-X %*% W)). We're leaving out intercept (bias).\n",
    "    layer_dense(units = 1, activation = 'sigmoid', use_bias = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs are of shape (? , 8) -- the model is agnostic about the number of samples but needs to know what shape to make the first layer. \n",
    "### layer_input (? x k) %*% layer_dense (k x 1) %>% sigmoid_function -> Output (? x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model <- keras_model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define our model using binary_crossentropy as the loss function to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model %>% compile(\n",
    "  loss = 'binary_crossentropy',\n",
    "  optimizer = optimizer_rmsprop(),\n",
    "  metrics = c('binary_crossentropy')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is binary crossentropy? It's the negative of the log likelihood of a Bernoulli distribution, which is exactly what glm in R is maximizing for logistic regression.\n",
    "\n",
    "\n",
    "$$binary \\ crossentropy = \\large - \\Sigma_{i=1}^{n} \\big( y_i \\ (log(\\hat{y_i})) + (1 - y_i) \\ log(1 - \\hat{y_i}) \\big)$$\n",
    "\n",
    "$$y_i = \\text{observation} \\ i$$\n",
    "$$\\hat{y_i} = \\text{predicted probability for observation} \\ i$$\n",
    "\n",
    "\n",
    "$$Bernoulli \\ likelihood \\ function = \\large \\Pi_{i=1}^{n} \\big( p^{x_i} \\ (1 - p)^{(1 - x_i)}    \\big) $$\n",
    "\n",
    "Taking the log of the above we get:\n",
    "\n",
    "$$ log(Bernoulli \\ likelihood \\ function)  = \\large \\Sigma_{i=1}^{n} \\big( x_i \\ log(p) + ( 1 - x_i)log(1-p) \\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- nn_model %>% fit(\n",
    "  X, Y, \n",
    "  epochs = 1000, batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the weights from the model. In our model there is only the input layer so we only see an array of dimension (k x 1), just as defined above in layer_dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<tbody>\n",
       "\t<tr><td> 0.130209595</td></tr>\n",
       "\t<tr><td> 0.013974152</td></tr>\n",
       "\t<tr><td>-0.029735258</td></tr>\n",
       "\t<tr><td> 0.001157426</td></tr>\n",
       "\t<tr><td> 0.001042823</td></tr>\n",
       "\t<tr><td>-0.003845388</td></tr>\n",
       "\t<tr><td> 0.324986279</td></tr>\n",
       "\t<tr><td>-0.014955019</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{tabular}{l}\n",
       "\t  0.130209595\\\\\n",
       "\t  0.013974152\\\\\n",
       "\t -0.029735258\\\\\n",
       "\t  0.001157426\\\\\n",
       "\t  0.001042823\\\\\n",
       "\t -0.003845388\\\\\n",
       "\t  0.324986279\\\\\n",
       "\t -0.014955019\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "|  0.130209595 | \n",
       "|  0.013974152 | \n",
       "| -0.029735258 | \n",
       "|  0.001157426 | \n",
       "|  0.001042823 | \n",
       "| -0.003845388 | \n",
       "|  0.324986279 | \n",
       "| -0.014955019 | \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "             [,1]\n",
       "[1,]  0.130209595\n",
       "[2,]  0.013974152\n",
       "[3,] -0.029735258\n",
       "[4,]  0.001157426\n",
       "[5,]  0.001042823\n",
       "[6,] -0.003845388\n",
       "[7,]  0.324986279\n",
       "[8,] -0.014955019\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_weights(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a logistic model to the data, leaving out the intercept as with the neural network. Just like with other machine learning methods, logistic regression in R is doing some optimization under the hood. Specifically, R is using [Iteratively reweighted least squares](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model <- glm(Y ~ X - 1, family = binomial(link='logit'), control = list(maxit = 25, epsilon=1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Xpregnant</dt>\n",
       "\t\t<dd>0.128418054493334</dd>\n",
       "\t<dt>Xglucose</dt>\n",
       "\t\t<dd>0.0129358346723977</dd>\n",
       "\t<dt>Xpressure</dt>\n",
       "\t\t<dd>-0.0303255465678681</dd>\n",
       "\t<dt>Xtriceps</dt>\n",
       "\t\t<dd>0.000195674544985645</dd>\n",
       "\t<dt>Xinsulin</dt>\n",
       "\t\t<dd>0.000738903841262783</dd>\n",
       "\t<dt>Xmass</dt>\n",
       "\t\t<dd>-0.00481362159208115</dd>\n",
       "\t<dt>Xpedigree</dt>\n",
       "\t\t<dd>0.320283774879749</dd>\n",
       "\t<dt>Xage</dt>\n",
       "\t\t<dd>-0.0156346467365025</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Xpregnant] 0.128418054493334\n",
       "\\item[Xglucose] 0.0129358346723977\n",
       "\\item[Xpressure] -0.0303255465678681\n",
       "\\item[Xtriceps] 0.000195674544985645\n",
       "\\item[Xinsulin] 0.000738903841262783\n",
       "\\item[Xmass] -0.00481362159208115\n",
       "\\item[Xpedigree] 0.320283774879749\n",
       "\\item[Xage] -0.0156346467365025\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Xpregnant\n",
       ":   0.128418054493334Xglucose\n",
       ":   0.0129358346723977Xpressure\n",
       ":   -0.0303255465678681Xtriceps\n",
       ":   0.000195674544985645Xinsulin\n",
       ":   0.000738903841262783Xmass\n",
       ":   -0.00481362159208115Xpedigree\n",
       ":   0.320283774879749Xage\n",
       ":   -0.0156346467365025\n",
       "\n"
      ],
      "text/plain": [
       "    Xpregnant      Xglucose     Xpressure      Xtriceps      Xinsulin \n",
       " 0.1284180545  0.0129358347 -0.0303255466  0.0001956745  0.0007389038 \n",
       "        Xmass     Xpedigree          Xage \n",
       "-0.0048136216  0.3202837749 -0.0156346467 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic_model$coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The coefficients (weights) from the neural network and logistic regression are very nearly identical. Any differences, might be attributable to the difference in number of training iterations or to the different optimization methods used in both models: [RMSprop](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) in the neural network and [Fisher's scoring](https://en.wikipedia.org/wiki/Scoring_algorithm) for glm in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0.0017915405</td></tr>\n",
       "\t<tr><td>0.0010383169</td></tr>\n",
       "\t<tr><td>0.0005902887</td></tr>\n",
       "\t<tr><td>0.0009617514</td></tr>\n",
       "\t<tr><td>0.0003039189</td></tr>\n",
       "\t<tr><td>0.0009682335</td></tr>\n",
       "\t<tr><td>0.0047025041</td></tr>\n",
       "\t<tr><td>0.0006796281</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t 0.0017915405\\\\\n",
       "\t 0.0010383169\\\\\n",
       "\t 0.0005902887\\\\\n",
       "\t 0.0009617514\\\\\n",
       "\t 0.0003039189\\\\\n",
       "\t 0.0009682335\\\\\n",
       "\t 0.0047025041\\\\\n",
       "\t 0.0006796281\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 0.0017915405 | \n",
       "| 0.0010383169 | \n",
       "| 0.0005902887 | \n",
       "| 0.0009617514 | \n",
       "| 0.0003039189 | \n",
       "| 0.0009682335 | \n",
       "| 0.0047025041 | \n",
       "| 0.0006796281 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]        \n",
       "[1,] 0.0017915405\n",
       "[2,] 0.0010383169\n",
       "[3,] 0.0005902887\n",
       "[4,] 0.0009617514\n",
       "[5,] 0.0003039189\n",
       "[6,] 0.0009682335\n",
       "[7,] 0.0047025041\n",
       "[8,] 0.0006796281"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_weights(nn_model)[[1]] - logistic_model$coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're still skeptical. Let's implement our neural network model from above from scratch, with one exception: instead of minimizing binary crossentropy we're going to maximize log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that \n",
    "\n",
    "$$binary \\ crossentropy = \\large - \\Sigma_{i=1}^{n} \\big( y_i \\ (log(\\hat{y_i})) + (1 - y_i) \\ log(1 - \\hat{y_i}) \\big)$$\n",
    "\n",
    "and since $\\hat{y_i} = \\frac{1}{1 + \\exp^{- X \\theta}}$, we obtain\n",
    "\n",
    "$$\\frac{\\partial \\ binary \\ crossentropy}{\\partial \\ \\theta} = \\large \\Sigma_{i=1}^{n} \\big( \\hat{y_i} - y_i \\big) x_i$$\n",
    "\n",
    "The derivative of the log-likelihood with respect to $\\theta$ is the negative of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>pregnant</th><td> 2.514683e-02</td></tr>\n",
       "\t<tr><th scope=row>glucose</th><td> 4.719363e-03</td></tr>\n",
       "\t<tr><th scope=row>pressure</th><td>-1.556096e-03</td></tr>\n",
       "\t<tr><th scope=row>triceps</th><td> 2.050325e-03</td></tr>\n",
       "\t<tr><th scope=row>insulin</th><td> 2.761012e-03</td></tr>\n",
       "\t<tr><th scope=row>mass</th><td> 9.211626e-04</td></tr>\n",
       "\t<tr><th scope=row>pedigree</th><td>-6.092262e-02</td></tr>\n",
       "\t<tr><th scope=row>age</th><td> 3.491333e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\tpregnant &  2.514683e-02\\\\\n",
       "\tglucose &  4.719363e-03\\\\\n",
       "\tpressure & -1.556096e-03\\\\\n",
       "\ttriceps &  2.050325e-03\\\\\n",
       "\tinsulin &  2.761012e-03\\\\\n",
       "\tmass &  9.211626e-04\\\\\n",
       "\tpedigree & -6.092262e-02\\\\\n",
       "\tage &  3.491333e-05\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| pregnant |  2.514683e-02 | \n",
       "| glucose |  4.719363e-03 | \n",
       "| pressure | -1.556096e-03 | \n",
       "| triceps |  2.050325e-03 | \n",
       "| insulin |  2.761012e-03 | \n",
       "| mass |  9.211626e-04 | \n",
       "| pedigree | -6.092262e-02 | \n",
       "| age |  3.491333e-05 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "         [,1]         \n",
       "pregnant  2.514683e-02\n",
       "glucose   4.719363e-03\n",
       "pressure -1.556096e-03\n",
       "triceps   2.050325e-03\n",
       "insulin   2.761012e-03\n",
       "mass      9.211626e-04\n",
       "pedigree -6.092262e-02\n",
       "age       3.491333e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n <- dim(X)[1]\n",
    "k <- dim(X)[2]\n",
    "\n",
    "\n",
    "sigmoid_fn <- function(z) {\n",
    "    return (1 / (1 + exp(-z)))\n",
    "}\n",
    "\n",
    "theta <- rep(0, k)\n",
    "\n",
    "\n",
    "# binary_crossentropy <- - sum(Y * log(y_hat) + (1 - Y) * log(1 - y_hat))\n",
    "# log_likelihood <- sum(Y * log(y_hat) + (1 - Y) * log(1 - y_hat))\n",
    "\n",
    "alpha <- 0.005\n",
    "beta <- 0.9\n",
    "\n",
    "v_dtheta <- 0\n",
    "\n",
    "for (epoch in 1:1000) {\n",
    "    \n",
    "    z <- X %*% theta\n",
    "    y_hat <- sigmoid_fn(z)\n",
    "    \n",
    "    # In a normal neural network we would use the derivative of binary crossentropy, but we're\n",
    "    # using the derivative of the log-likelihood here.\n",
    "    \n",
    "    # deriv_binary_crossentropy <- t(X) %*% (y_hat - Y)\n",
    "    deriv_log_likelihood <- - t(X) %*% (y_hat - Y)\n",
    "    \n",
    "    # The following implements RMSProp which was used in the neural network model above.\n",
    "    \n",
    "    # v_dtheta <- beta * v_dtheta + (1 - beta) * deriv_binary_crossentropy^2\n",
    "    v_dtheta <- beta * v_dtheta + (1 - beta) * deriv_log_likelihood^2\n",
    "    \n",
    "    # new_theta <- new_theta - alpha * deriv_binary_crossentropy / sqrt(v_dtheta)\n",
    "    theta <- theta + alpha * deriv_log_likelihood / sqrt(v_dtheta + 1e-10)\n",
    "}\n",
    "\n",
    "theta - logistic_model$coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: If you've used logistic regression in R before, you've used a neural network! (Though you probably shouldn't change your LinkedIn profile to Deep Learning Engineer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: for an in-depth illustration of what's going on when you call glm in R, see the Fisher scoring method below. For more detail, read [this paper](https://statacumen.com/teach/SC1/SC1_11_LogisticRegression.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_logistic_regression = function(X,y,threshold = 1e-10, max_iter = 100) {\n",
    "  #A function to find logistic regression coeffiecients \n",
    "  #Takes three inputs:\n",
    "    \n",
    "    \n",
    "  #A function to return p, given X and beta\n",
    "  #We'll need this function in the iterative section\n",
    "#   sigmoid = function(X, beta) {\n",
    "#     beta = as.vector(beta)\n",
    "#     return((1) / (1 + exp(-(X %*% beta))))\n",
    "#   }  \n",
    " \n",
    "  #### setup bit ####\n",
    " \n",
    "  #initial guess for beta\n",
    "  beta = rep(0, ncol(X))\n",
    " \n",
    "  #initial value bigger than threshold so that we can enter our while loop \n",
    "  diff = 10000 \n",
    " \n",
    "  #counter to ensure we're not stuck in an infinite loop\n",
    "  iter_count = 0\n",
    "  \n",
    "  #### iterative bit ####\n",
    "  while(diff > threshold ) #tests for convergence\n",
    "  {\n",
    "    # Calculate probability vector using sigmoid_fn defined above\n",
    "    p = as.vector(sigmoid_fn(X %*% beta))\n",
    "    \n",
    "    #calculate matrix of weights W\n",
    "    W =  diag(p*(1-p)) \n",
    "    \n",
    "    #calculate the change in beta\n",
    "    beta_change = solve(t(X)%*%W%*%X) %*% t(X)%*%(y - p)\n",
    "    \n",
    "    #update beta\n",
    "    beta = beta + beta_change\n",
    "    \n",
    "    #calculate how much we changed beta by in this iteration \n",
    "    #if this is less than threshold, we'll break the while loop \n",
    "    diff = sum(beta_change^2)\n",
    "    \n",
    "    #see if we've hit the maximum number of iterations\n",
    "    iter_count = iter_count + 1\n",
    "\n",
    "  }\n",
    "\n",
    "  print(iter_count)\n",
    "  return(beta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'iter_count' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'iter_count' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>pregnant</th><td>-2.302855e-11</td></tr>\n",
       "\t<tr><th scope=row>glucose</th><td>-3.585189e-12</td></tr>\n",
       "\t<tr><th scope=row>pressure</th><td> 1.035030e-11</td></tr>\n",
       "\t<tr><th scope=row>triceps</th><td> 9.101347e-13</td></tr>\n",
       "\t<tr><th scope=row>insulin</th><td> 1.779653e-13</td></tr>\n",
       "\t<tr><th scope=row>mass</th><td>-7.169608e-12</td></tr>\n",
       "\t<tr><th scope=row>pedigree</th><td>-4.444600e-11</td></tr>\n",
       "\t<tr><th scope=row>age</th><td> 1.446003e-12</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\tpregnant & -2.302855e-11\\\\\n",
       "\tglucose & -3.585189e-12\\\\\n",
       "\tpressure &  1.035030e-11\\\\\n",
       "\ttriceps &  9.101347e-13\\\\\n",
       "\tinsulin &  1.779653e-13\\\\\n",
       "\tmass & -7.169608e-12\\\\\n",
       "\tpedigree & -4.444600e-11\\\\\n",
       "\tage &  1.446003e-12\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| pregnant | -2.302855e-11 | \n",
       "| glucose | -3.585189e-12 | \n",
       "| pressure |  1.035030e-11 | \n",
       "| triceps |  9.101347e-13 | \n",
       "| insulin |  1.779653e-13 | \n",
       "| mass | -7.169608e-12 | \n",
       "| pedigree | -4.444600e-11 | \n",
       "| age |  1.446003e-12 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "         [,1]         \n",
       "pregnant -2.302855e-11\n",
       "glucose  -3.585189e-12\n",
       "pressure  1.035030e-11\n",
       "triceps   9.101347e-13\n",
       "insulin   1.779653e-13\n",
       "mass     -7.169608e-12\n",
       "pedigree -4.444600e-11\n",
       "age       1.446003e-12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manual_logistic_regression(X,Y) - logistic_model$coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_theta <- rep(0, k)  # Initialize theta (weights, coefficient, beta, etc.)\n",
    "fisher_theta_update <- rep(1, k)  # Initialize beta_update\n",
    "epsilon <- 1e-10\n",
    "\n",
    "### Fit logistic regression using Fisher scoring ###\n",
    "\n",
    "# While the difference between theta and theta update is greater than some threshold epsilon\n",
    "while (sum(abs(fisher_theta - fisher_theta_update)) > epsilon) {\n",
    "    \n",
    "    # Assign theta to theta_update\n",
    "    fisher_theta_update <- fisher_theta\n",
    "\n",
    "    # Calculate mu and eta\n",
    "    eta <- X %*% fisher_theta\n",
    "    mu <- 1 / (1 + exp(-eta))\n",
    "\n",
    "    # Calculate the derivatives\n",
    "    #dmu_deta <- exp(-eta) * (1 / (( 1 + exp(-eta)) ^ 2))\n",
    "    dmu_deta <- mu * (1 - mu)\n",
    "    deta_dmu <- 1 / mu + 1 / (1 - mu)\n",
    "\n",
    "    # Calculate the variance function. Recall that the variance of the Bernoulli distribution is p * (1 - p)\n",
    "    # Since in the logistic regression setting p = 1 / (1 + (-X %*% theta)), the variance vector is defined as:\n",
    "    V <- mu * (1 - mu) \n",
    "    \n",
    "    # Calculate the weight matrix\n",
    "    W <- diag((dmu_deta ^ 2 / V)[,]) \n",
    "    Z <- eta + (Y - mu) * deta_dmu  # Calculate Z\n",
    "\n",
    "    # theta = (X^T %*% W %*% X)^(-1) %*% (X^T %*% W %*% Z)\n",
    "    fisher_theta <- solve(t(X) %*% W %*% X) %*% (t(X) %*% W %*% Z)\n",
    "    #fisher_theta <- solve(t(X) %*% W %*% X) %*% (t(X) %*% (Y - mu))\n",
    "}\n",
    "\n",
    "fisher_theta - logistic_model$coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta + (Y - mu) * deta_dmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_theta <- k_zeros(shape = c(0, k))  # Initialize theta (weights, coefficient, beta, etc.)\n",
    "fisher_theta_update <- k_ones(shape = c(0, k))  # Initialize beta_update\n",
    "epsilon <- k_eps()\n",
    "\n",
    "while (sum(abs(fisher_theta - fisher_theta_update)) > epsilon) {\n",
    "    \n",
    "    # Assign theta to theta_update\n",
    "    fisher_theta_update <- fisher_theta\n",
    "\n",
    "    # Calculate mu and eta\n",
    "    #eta <- X %*% fisher_theta\n",
    "    eta <- k_dot(X, fisher_theta)\n",
    "    \n",
    "    mu <- k_sigmoid(eta)\n",
    "    # mu <- 1 / (1 + exp(-eta))\n",
    "\n",
    "    # Calculate the derivatives\n",
    "    #dmu_deta <- exp(-eta) * (1 / (( 1 + exp(-eta)) ^ 2))\n",
    "    dmu_deta <- \n",
    "    deta_dmu <- 1 / mu + 1 / (1 - mu)\n",
    "\n",
    "    # Calculate the variance function. Recall that the variance of the Bernoulli distribution is p * (1 - p)\n",
    "    # Since in the logistic regression setting p = 1 / (1 + (-X %*% theta)), the variance vector is defined as:\n",
    "    V <- mu * (1 - mu) \n",
    "    \n",
    "    # Calculate the weight matrix\n",
    "    W <- diag((dmu_deta ^ 2 / V)[,]) \n",
    "    Z <- eta + (Y - mu) * deta_dmu  # Calculate Z\n",
    "\n",
    "    # theta = (X^T %*% W %*% X)^(-1) %*% (X^T %*% W %*% Z)\n",
    "    fisher_theta <- solve(t(X) %*% W %*% X) %*% (t(X) %*% W %*% Z)\n",
    "}\n",
    "\n",
    "fisher_theta - logistic_model$coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 <- k_sigmoid(eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full disclosure: all of the hyperparameters were carefully chosen to produce the same weights in the NN model as the logistic regression model. It is by no means guaranteed to get similar weights for the 2 types of models across different hyperparameters or even across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If it seems like we're far away from neural networks, consider what we've done:\n",
    "    1. We defined a neural network with no hidden layer.\n",
    "    2. We are using a form of backpropagation\n",
    "    3. Our loss function is just binary crossentropy with a sign reversal.\n",
    "    4. We use a particular form of optimization, rather than gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
